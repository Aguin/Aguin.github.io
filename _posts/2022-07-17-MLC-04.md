---
title: "[MLC-机器学习编译]04 端到端模型整合"
date: 2022-07-17
categories:
  - Notes
tags:
  - MLC
classes: wide
---

{% include video id="BV1Lf4y1o7xM" provider="bilibili" danmaku="1" %}

[official notes (中文)](https://mlc.ai/zh/chapter_end_to_end/index.html){: .btn .btn--primary}

## 笔记
- 用两个linear层的Fasion-MNIST模型来解释`relax`.
- `call_tir`中的一些设计:
  - `destination passing`: 将输出也作为输入传入, 方便内存管理(让高层库去做内存操作, 底层库就不用涉及内存), 但是和传统的计算图不太匹配(接受输入返回输出). `call_tir`期望底层实现能和计算图之间看起来更加一致.
  - `pure` or `side-effect free`: 只从输入中读取并输出返回结果，不改变程序的其他部分. `call_tir`希望隐藏可能的分配或对函数的显式写入.
- `dataflow`标注计算图的范围, 计算图之外的操作可能包含side-effect.
- `from_dlpack`将`TVM NDArray`转换为`torch NDArray`(零拷贝).
- `IRModule`支持注册runtime functin, 也可以与`TensorIR`混用.
- 可以使用`relax.transform.BindParams`绑定参数, 类似于`functools.partial`.
- 这节课主要想传达不同的表示形式可能会影响后续优化的难度.